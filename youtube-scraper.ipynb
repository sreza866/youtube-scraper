{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "278f28bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "77349ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. Write a python program to extract the video URL of the first five videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acf41b9",
   "metadata": {},
   "source": [
    "### Solution1-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "60d1ff03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Five Unique Video URLs:\n",
      "Video 1 URL: https://www.youtube.com/watch?v=mztz_H1alAE\n",
      "Video 2 URL: https://www.youtube.com/watch?v=MwVGbIC5qj0\n",
      "Video 3 URL: https://www.youtube.com/watch?v=9Yt1qj_Bx4Y\n",
      "Video 4 URL: https://www.youtube.com/watch?v=gkguIT1XCo4\n",
      "Video 5 URL: https://www.youtube.com/watch?v=CCw6oto2WlA\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_video_urls(channel_url):\n",
    "    # Initialize Chrome webdriver\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    # Open the YouTube channel URL\n",
    "    driver.get(channel_url)\n",
    "\n",
    "    # Wait for the videos to load\n",
    "    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, \"content\")))\n",
    "\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content, \"lxml\")\n",
    "\n",
    "    # Find the container holding all the video links\n",
    "    container = soup.find(\"div\", id=\"content\")\n",
    "\n",
    "    # Find all 'a' tags within the container\n",
    "    video_links = container.find_all(\"a\")\n",
    "\n",
    "    # List to store unique video URLs\n",
    "    unique_video_urls = []\n",
    "\n",
    "    # Get the first five unique video URLs\n",
    "    for link in video_links:\n",
    "        if len(unique_video_urls) >= 5:\n",
    "            break\n",
    "        if 'href' in link.attrs and \"/watch?v=\" in link['href']:\n",
    "            video_url = \"https://www.youtube.com\" + link['href']\n",
    "            if video_url not in unique_video_urls:\n",
    "                unique_video_urls.append(video_url)\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    return unique_video_urls\n",
    "\n",
    "def main():\n",
    "    channel_url = \"https://www.youtube.com/@PW-Foundation/videos\"\n",
    "    video_urls = extract_video_urls(channel_url)\n",
    "\n",
    "    # Print unique video URLs\n",
    "    print(\"First Five Unique Video URLs:\")\n",
    "    for index, video_url in enumerate(video_urls, start=1):\n",
    "        print(f\"Video {index} URL:\", video_url)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd3385e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. Write a python program to extract the URL of the video thumbnails of the first five videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1d4f4a",
   "metadata": {},
   "source": [
    "### Solution 2-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "1648f084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Five Thumbnail URLs:\n",
      "Thumbnail 1 URL: /watch?v=mztz_H1alAE\n",
      "Thumbnail 2 URL: /watch?v=MwVGbIC5qj0\n",
      "Thumbnail 3 URL: /watch?v=9Yt1qj_Bx4Y\n",
      "Thumbnail 4 URL: /watch?v=gkguIT1XCo4\n",
      "Thumbnail 5 URL: /watch?v=CCw6oto2WlA\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def extract_thumbnail_urls(channel_url):\n",
    "    # Initialize Chrome webdriver\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    # Open the YouTube channel URL\n",
    "    driver.get(channel_url)\n",
    "\n",
    "    # Wait for the videos to load\n",
    "    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, \"content\")))\n",
    "\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content, \"lxml\")\n",
    "\n",
    "    # Find all thumbnail anchors\n",
    "    thumbnail_anchors = soup.find_all('div', id='thumbnail')\n",
    "\n",
    "    # Extract thumbnail URLs of the first five videos\n",
    "    thumbnail_urls = []\n",
    "    for thumbnail_anchor in thumbnail_anchors:\n",
    "        if len(thumbnail_urls) >= 5:\n",
    "            break\n",
    "        anchor = thumbnail_anchor.find('a')\n",
    "        if anchor and 'href' in anchor.attrs:\n",
    "            thumbnail_url = anchor['href']\n",
    "            thumbnail_urls.append(thumbnail_url)\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    return thumbnail_urls\n",
    "\n",
    "\n",
    "def main():\n",
    "  channel_url = \"https://www.youtube.com/@PW-Foundation/videos\"\n",
    "  thumbnails = extract_thumbnail_urls(channel_url)\n",
    "\n",
    "  # Print unique thumbnail URLs\n",
    "  print(\"First Five Thumbnail URLs:\")\n",
    "  for index, thumbnail in enumerate(thumbnails, start=1):\n",
    "    print(f\"Thumbnail {index} URL:\", thumbnail)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8412c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. Write a python program to extract the title of the first five videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4560a03c",
   "metadata": {},
   "source": [
    "### Solution 3-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "08de62da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Five Video Titles:\n",
      "Video 1 Title: How to Start Class 10th to Score 98%? 😎 Avoid These Mistakes 🤯\n",
      "Video 2 Title: Meet Saifur Raza : Scored 96% in Class 10th UP Board Exam 2024 🔥\n",
      "Video 3 Title: What's Inside PW Gurukulam School 😍 | Trailer | @PWGurukulam\n",
      "Video 4 Title: CBSE Class 10th Result OUT or NOT ?? || CBSE Latest News || Result कब आने वाला है? 🤯\n",
      "Video 5 Title: Meet Mahi Kanwal : Scored 96.2% in Class 10th UK Board Exam 2024 🔥\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def extract_video_titles(channel_url):\n",
    "    # Initialize Chrome webdriver\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    # Open the YouTube channel URL\n",
    "    driver.get(channel_url)\n",
    "\n",
    "    # Wait for the videos to load\n",
    "    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, \"video-title\")))\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content, \"lxml\")\n",
    "    \n",
    "    # Find all video elements\n",
    "    video_title_anchors = soup.find_all('a', class_='yt-simple-endpoint')\n",
    "    video_title_anchors = video_title_anchors[3:]  \n",
    "\n",
    "    # Extract the titles of the first five videos\n",
    "    video_titles = []\n",
    "    for index, anchor in enumerate(video_title_anchors, start=3):\n",
    "        if len(video_titles) >= 5:\n",
    "            break\n",
    "        if index > 8:\n",
    "            title = anchor.get('title')\n",
    "            if title and title != 'undefined':\n",
    "                video_titles.append(title)\n",
    "    \n",
    "    # Close the webdriver\n",
    "    driver.quit()\n",
    "    \n",
    "    return video_titles\n",
    "\n",
    "def main():\n",
    "    channel_url = \"https://www.youtube.com/@PW-Foundation/videos\"\n",
    "    video_titles = extract_video_titles(channel_url)\n",
    "    \n",
    "    # Print the extracted video titles\n",
    "    print(\"First Five Video Titles:\")\n",
    "    for index, title in enumerate(video_titles, start=1):\n",
    "        print(f\"Video {index} Title:\", title)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9db086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. Write a python program to extract the number of views of the first five videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ec12c9",
   "metadata": {},
   "source": [
    "### Solution 4-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b21a0642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Views:\n",
      "Video 1 Views: 13,075\n",
      "Video 2 Views: 6,015\n",
      "Video 3 Views: 37,819\n",
      "Video 4 Views: 374,603\n",
      "Video 5 Views: 7,609\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def extract_video_views(channel_url):\n",
    "    # Initialize Chrome webdriver\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    # Open the YouTube channel URL\n",
    "    driver.get(channel_url)\n",
    "\n",
    "    # Wait for the videos to load\n",
    "    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, \"video-title\")))\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content, \"lxml\")\n",
    "    \n",
    "    # Find all video elements\n",
    "    video_title_anchors = soup.find_all('a', class_='yt-simple-endpoint')\n",
    "\n",
    "    # Extract the number of views from the aria-label attribute\n",
    "    video_views = []\n",
    "    for index, anchor in enumerate(video_title_anchors, start=1):\n",
    "        if len(video_views) >= 5:\n",
    "            break\n",
    "        if index > 8:\n",
    "            aria_label = anchor.get('aria-label')\n",
    "            if aria_label:\n",
    "                # Extract the number of views from the aria-label\n",
    "                views_start_index = aria_label.find(' views')\n",
    "                if views_start_index != -1:\n",
    "                    views_text = aria_label[:views_start_index]\n",
    "                    views_text_split = views_text.split(' ')\n",
    "                    views = views_text_split[-1]\n",
    "                    video_views.append(views)\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    return video_views\n",
    "\n",
    "def main():\n",
    "    channel_url = \"https://www.youtube.com/@PW-Foundation/videos\"\n",
    "    video_views = extract_video_views(channel_url)\n",
    "    \n",
    "    # Print the extracted video views\n",
    "    print(\"Video Views:\")\n",
    "    for index, views in enumerate(video_views, start=1):\n",
    "        if views != \"Views not found\":\n",
    "            print(f\"Video {index} Views:\", views)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d26b956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Write a python program to extract the time of posting of video for the first five videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3c26c6",
   "metadata": {},
   "source": [
    "### Solution 5-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a007c5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Five Video Posting Time:\n",
      "Video 1 Posting Time: 1 day ago\n",
      "Video 2 Posting Time: 2 days ago\n",
      "Video 3 Posting Time: 3 days ago\n",
      "Video 4 Posting Time: 3 days ago\n",
      "Video 5 Posting Time: 4 days ago\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def extract_video_posting_time(channel_url):\n",
    "    # Initialize Chrome webdriver\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    # Open the YouTube channel URL\n",
    "    driver.get(channel_url)\n",
    "\n",
    "    # Wait for the videos to load\n",
    "    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, \"video-title\")))\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content, \"lxml\")\n",
    "    \n",
    "    # Find all video elements\n",
    "    video_title_anchors = soup.find_all('a', class_='yt-simple-endpoint')\n",
    "\n",
    "    # Extract the posting time from the aria-label attribute for the first five videos\n",
    "    video_posting_time = []\n",
    "    for index, anchor in enumerate(video_title_anchors, start=1):\n",
    "        if len(video_posting_time) >= 5:\n",
    "            break\n",
    "        if index > 8:\n",
    "            aria_label = anchor.get('aria-label')\n",
    "            if aria_label:\n",
    "                # Use regular expressions to extract the posting time\n",
    "                time_match = re.search(r'\\d+ (hour|minute|day|week|month|year)s? ago', aria_label)\n",
    "                if time_match:\n",
    "                    video_posting_time.append(time_match.group())\n",
    "    \n",
    "    # Close the webdriver\n",
    "    driver.quit()\n",
    "    \n",
    "    return video_posting_time\n",
    "\n",
    "def main():\n",
    "    channel_url = \"https://www.youtube.com/@PW-Foundation/videos\"\n",
    "    video_posting_time = extract_video_posting_time(channel_url)\n",
    "    \n",
    "    # Print the extracted video posting time\n",
    "    print(\"First Five Video Posting Time:\")\n",
    "    for index, time in enumerate(video_posting_time, start=1):\n",
    "        print(f\"Video {index} Posting Time:\", time)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab234f0",
   "metadata": {},
   "source": [
    "###  Function for getting all the details together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "08e67ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Five Unique Video URLs:\n",
      "Video 1 URL: https://www.youtube.com/watch?v=mztz_H1alAE\n",
      "Video 2 URL: https://www.youtube.com/watch?v=MwVGbIC5qj0\n",
      "Video 3 URL: https://www.youtube.com/watch?v=9Yt1qj_Bx4Y\n",
      "Video 4 URL: https://www.youtube.com/watch?v=gkguIT1XCo4\n",
      "Video 5 URL: https://www.youtube.com/watch?v=CCw6oto2WlA\n",
      "\n",
      "First Five Thumbnail URLs:\n",
      "Video 1 Thumbnail URL: /watch?v=mztz_H1alAE\n",
      "Video 2 Thumbnail URL: /watch?v=MwVGbIC5qj0\n",
      "Video 3 Thumbnail URL: /watch?v=9Yt1qj_Bx4Y\n",
      "Video 4 Thumbnail URL: /watch?v=gkguIT1XCo4\n",
      "Video 5 Thumbnail URL: /watch?v=CCw6oto2WlA\n",
      "\n",
      "First Five Video Titles:\n",
      "Video 1 Title: How to Start Class 10th to Score 98%? 😎 Avoid These Mistakes 🤯\n",
      "Video 2 Title: Meet Saifur Raza : Scored 96% in Class 10th UP Board Exam 2024 🔥\n",
      "Video 3 Title: What's Inside PW Gurukulam School 😍 | Trailer | @PWGurukulam\n",
      "Video 4 Title: CBSE Class 10th Result OUT or NOT ?? || CBSE Latest News || Result कब आने वाला है? 🤯\n",
      "Video 5 Title: Meet Mahi Kanwal : Scored 96.2% in Class 10th UK Board Exam 2024 🔥\n",
      "\n",
      "First Five Video Views:\n",
      "Video 1 Views: 13,076\n",
      "Video 2 Views: 6,015\n",
      "Video 3 Views: 37,819\n",
      "Video 4 Views: 374,604\n",
      "Video 5 Views: 7,609\n",
      "\n",
      "First Five Video Posting Time:\n",
      "Video 1 Posting Time: 1 day ago\n",
      "Video 2 Posting Time: 2 days ago\n",
      "Video 3 Posting Time: 3 days ago\n",
      "Video 4 Posting Time: 3 days ago\n",
      "Video 5 Posting Time: 4 days ago\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def extract_video_urls(channel_url):\n",
    "    # Initialize Chrome webdriver\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    # Open the YouTube channel URL\n",
    "    driver.get(channel_url)\n",
    "\n",
    "    # Wait for the videos to load\n",
    "    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, \"content\")))\n",
    "\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content, \"lxml\")\n",
    "\n",
    "    # Find the container holding all the video links\n",
    "    container = soup.find(\"div\", id=\"content\")\n",
    "\n",
    "    # Find all 'a' tags within the container\n",
    "    video_links = container.find_all(\"a\")\n",
    "\n",
    "    # List to store unique video URLs\n",
    "    unique_video_urls = []\n",
    "\n",
    "    # Get the first five unique video URLs\n",
    "    for link in video_links:\n",
    "        if len(unique_video_urls) >= 5:\n",
    "            break\n",
    "        if 'href' in link.attrs and \"/watch?v=\" in link['href']:\n",
    "            video_url = \"https://www.youtube.com\" + link['href']\n",
    "            if video_url not in unique_video_urls:\n",
    "                unique_video_urls.append(video_url)\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    return unique_video_urls\n",
    "\n",
    "\n",
    "\n",
    "def extract_thumbnail_urls(channel_url):\n",
    "    # Initialize Chrome webdriver\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    # Open the YouTube channel URL\n",
    "    driver.get(channel_url)\n",
    "\n",
    "    # Wait for the videos to load\n",
    "    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, \"content\")))\n",
    "\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content, \"lxml\")\n",
    "\n",
    "    # Find all thumbnail anchors\n",
    "    thumbnail_anchors = soup.find_all('div', id='thumbnail')\n",
    "\n",
    "    # Extract thumbnail URLs of the first five videos\n",
    "    thumbnail_urls = []\n",
    "    for thumbnail_anchor in thumbnail_anchors:\n",
    "        if len(thumbnail_urls) >= 5:\n",
    "            break\n",
    "        anchor = thumbnail_anchor.find('a')\n",
    "        if anchor and 'href' in anchor.attrs:\n",
    "            thumbnail_url = anchor['href']\n",
    "            thumbnail_urls.append(thumbnail_url)\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    return thumbnail_urls\n",
    "\n",
    "\n",
    "\n",
    "def extract_video_titles(channel_url):\n",
    "    # Initialize Chrome webdriver\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    # Open the YouTube channel URL\n",
    "    driver.get(channel_url)\n",
    "\n",
    "    # Wait for the videos to load\n",
    "    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, \"video-title\")))\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content, \"lxml\")\n",
    "    \n",
    "    # Find all video elements\n",
    "    video_title_anchors = soup.find_all('a', class_='yt-simple-endpoint')\n",
    "    video_title_anchors = video_title_anchors[3:]  \n",
    "\n",
    "    # Extract the titles of the first five videos\n",
    "    video_titles = []\n",
    "    for index, anchor in enumerate(video_title_anchors, start=3):\n",
    "        if len(video_titles) >= 5:\n",
    "            break\n",
    "        if index > 8:\n",
    "            title = anchor.get('title')\n",
    "            if title and title != 'undefined':\n",
    "                video_titles.append(title)\n",
    "    \n",
    "    # Close the webdriver\n",
    "    driver.quit()\n",
    "    \n",
    "    return video_titles\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_video_views(channel_url):\n",
    "    # Initialize Chrome webdriver\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    # Open the YouTube channel URL\n",
    "    driver.get(channel_url)\n",
    "\n",
    "    # Wait for the videos to load\n",
    "    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, \"video-title\")))\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content, \"lxml\")\n",
    "    \n",
    "    # Find all video elements\n",
    "    video_title_anchors = soup.find_all('a', class_='yt-simple-endpoint')\n",
    "\n",
    "    # Extract the number of views from the aria-label attribute\n",
    "    video_views = []\n",
    "    for index, anchor in enumerate(video_title_anchors, start=1):\n",
    "        if len(video_views) >= 5:\n",
    "            break\n",
    "        if index > 8:\n",
    "            aria_label = anchor.get('aria-label')\n",
    "            if aria_label:\n",
    "                # Extract the number of views from the aria-label\n",
    "                views_start_index = aria_label.find(' views')\n",
    "                if views_start_index != -1:\n",
    "                    views_text = aria_label[:views_start_index]\n",
    "                    views_text_split = views_text.split(' ')\n",
    "                    views = views_text_split[-1]\n",
    "                    video_views.append(views)\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    return video_views\n",
    "\n",
    "\n",
    "\n",
    "def extract_video_posting_time(channel_url):\n",
    "    # Initialize Chrome webdriver\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    # Open the YouTube channel URL\n",
    "    driver.get(channel_url)\n",
    "\n",
    "    # Wait for the videos to load\n",
    "    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, \"video-title\")))\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content, \"lxml\")\n",
    "    \n",
    "    # Find all video elements\n",
    "    video_title_anchors = soup.find_all('a', class_='yt-simple-endpoint')\n",
    "\n",
    "    # Extract the posting time from the aria-label attribute for the first five videos\n",
    "    video_posting_time = []\n",
    "    for index, anchor in enumerate(video_title_anchors, start=1):\n",
    "        if len(video_posting_time) >= 5:\n",
    "            break\n",
    "        if index > 8:\n",
    "            aria_label = anchor.get('aria-label')\n",
    "            if aria_label:\n",
    "                # Use regular expressions to extract the posting time\n",
    "                time_match = re.search(r'\\d+ (hour|minute|day|week|month|year)s? ago', aria_label)\n",
    "                if time_match:\n",
    "                    video_posting_time.append(time_match.group())\n",
    "    \n",
    "    # Close the webdriver\n",
    "    driver.quit()\n",
    "    \n",
    "    return video_posting_time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    channel_url = \"https://www.youtube.com/@PW-Foundation/videos\"\n",
    "    \n",
    "    # Extract video URLs\n",
    "    video_urls = extract_video_urls(channel_url)\n",
    "\n",
    "    # Print unique video URLs\n",
    "    print(\"First Five Unique Video URLs:\")\n",
    "    for index, video_url in enumerate(video_urls, start=1):\n",
    "        print(f\"Video {index} URL:\", video_url)\n",
    "    print()\n",
    "    \n",
    "    # Extract thumbnail URLs\n",
    "    thumbnail_urls = extract_thumbnail_urls(channel_url)\n",
    "    print(\"First Five Thumbnail URLs:\")\n",
    "    for index, url in enumerate(thumbnail_urls, start=1):\n",
    "        print(f\"Video {index} Thumbnail URL:\", url)\n",
    "    print()\n",
    "    \n",
    "    # Extract video titles\n",
    "    video_titles = extract_video_titles(channel_url)\n",
    "    # Print the extracted video titles\n",
    "    print(\"First Five Video Titles:\")\n",
    "    for index, title in enumerate(video_titles, start=1):\n",
    "        print(f\"Video {index} Title:\", title)\n",
    "    print()\n",
    "    \n",
    "    # Extract video views\n",
    "    video_views = extract_video_views(channel_url)\n",
    "    print(\"First Five Video Views:\")\n",
    "    for index, views in enumerate(video_views, start=1):\n",
    "        print(f\"Video {index} Views:\", views)\n",
    "    print()\n",
    "    \n",
    "    # Print the extracted video posting time\n",
    "    video_posting_time = extract_video_posting_time(channel_url)\n",
    "    print(\"First Five Video Posting Time:\")\n",
    "    for index, time in enumerate(video_posting_time, start=1):\n",
    "        print(f\"Video {index} Posting Time:\", time)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3554cf",
   "metadata": {},
   "source": [
    "### Note: Save all the data scraped in the above questions in a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "731fe91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def extract_video_urls(channel_url):\n",
    "    # Initialize Chrome webdriver\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Open the YouTube channel URL\n",
    "    driver.get(channel_url)\n",
    "\n",
    "    # Wait for the videos to load\n",
    "    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, \"content\")))\n",
    "\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content, \"lxml\")\n",
    "\n",
    "    # Find the container holding all the video links\n",
    "    container = soup.find(\"div\", id=\"content\")\n",
    "\n",
    "    # Find all 'a' tags within the container\n",
    "    video_links = container.find_all(\"a\")\n",
    "\n",
    "    # List to store unique video URLs\n",
    "    unique_video_urls = []\n",
    "\n",
    "    # Get the first five unique video URLs\n",
    "    for link in video_links:\n",
    "        if len(unique_video_urls) >= 5:\n",
    "            break\n",
    "        if 'href' in link.attrs and \"/watch?v=\" in link['href']:\n",
    "            video_url = \"https://www.youtube.com\" + link['href']\n",
    "            if video_url not in unique_video_urls:\n",
    "                unique_video_urls.append(video_url)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return unique_video_urls\n",
    "\n",
    "\n",
    "def extract_thumbnail_urls(channel_url):\n",
    "    # Initialize Chrome webdriver\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Open the YouTube channel URL\n",
    "    driver.get(channel_url)\n",
    "\n",
    "    # Wait for the videos to load\n",
    "    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, \"content\")))\n",
    "\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content, \"lxml\")\n",
    "\n",
    "    # Find all thumbnail anchors\n",
    "    thumbnail_anchors = soup.find_all('div', id='thumbnail')\n",
    "\n",
    "    # Extract thumbnail URLs of the first five videos\n",
    "    thumbnail_urls = []\n",
    "    for thumbnail_anchor in thumbnail_anchors:\n",
    "        if len(thumbnail_urls) >= 5:\n",
    "            break\n",
    "        anchor = thumbnail_anchor.find('a')\n",
    "        if anchor and 'href' in anchor.attrs:\n",
    "            thumbnail_url = anchor['href']\n",
    "            thumbnail_urls.append(thumbnail_url)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return thumbnail_urls\n",
    "\n",
    "\n",
    "def extract_video_titles(channel_url):\n",
    "    # Initialize Chrome webdriver\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Open the YouTube channel URL\n",
    "    driver.get(channel_url)\n",
    "\n",
    "    # Wait for the videos to load\n",
    "    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, \"video-title\")))\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content, \"lxml\")\n",
    "\n",
    "    # Find all video elements\n",
    "    video_title_anchors = soup.find_all('a', class_='yt-simple-endpoint')\n",
    "    video_title_anchors = video_title_anchors[3:]\n",
    "\n",
    "    # Extract the titles of the first five videos\n",
    "    video_titles = []\n",
    "    for index, anchor in enumerate(video_title_anchors, start=3):\n",
    "        if len(video_titles) >= 5:\n",
    "            break\n",
    "        if index > 8:\n",
    "            title = anchor.get('title')\n",
    "            if title and title != 'undefined':\n",
    "                video_titles.append(title)\n",
    "\n",
    "    # Close the webdriver\n",
    "    driver.quit()\n",
    "\n",
    "    return video_titles\n",
    "\n",
    "\n",
    "def extract_video_views(channel_url):\n",
    "    # Initialize Chrome webdriver\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Open the YouTube channel URL\n",
    "    driver.get(channel_url)\n",
    "\n",
    "    # Wait for the videos to load\n",
    "    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, \"video-title\")))\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content, \"lxml\")\n",
    "\n",
    "    # Find all video elements\n",
    "    video_title_anchors = soup.find_all('a', class_='yt-simple-endpoint')\n",
    "\n",
    "    # Extract the number of views from the aria-label attribute\n",
    "    video_views = []\n",
    "    for index, anchor in enumerate(video_title_anchors, start=1):\n",
    "        if len(video_views) >= 5:\n",
    "            break\n",
    "        if index > 8:\n",
    "            aria_label = anchor.get('aria-label')\n",
    "            if aria_label:\n",
    "                # Extract the number of views from the aria-label\n",
    "                views_start_index = aria_label.find(' views')\n",
    "                if views_start_index != -1:\n",
    "                    views_text = aria_label[:views_start_index]\n",
    "                    views_text_split = views_text.split(' ')\n",
    "                    views = views_text_split[-1]\n",
    "                    video_views.append(views)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return video_views\n",
    "\n",
    "\n",
    "def extract_video_posting_time(channel_url):\n",
    "    # Initialize Chrome webdriver\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Open the YouTube channel URL\n",
    "    driver.get(channel_url)\n",
    "\n",
    "    # Wait for the videos to load\n",
    "    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, \"video-title\")))\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content, \"lxml\")\n",
    "\n",
    "    # Find all video elements\n",
    "    video_title_anchors = soup.find_all('a', class_='yt-simple-endpoint')\n",
    "\n",
    "    # Extract the posting time from the aria-label attribute for the first five videos\n",
    "    video_posting_time = []\n",
    "    for index, anchor in enumerate(video_title_anchors, start=1):\n",
    "        if len(video_posting_time) >= 5:\n",
    "            break\n",
    "        if index > 8:\n",
    "            aria_label = anchor.get('aria-label')\n",
    "            if aria_label:\n",
    "                # Use regular expressions to extract the posting time\n",
    "                time_match = re.search(r'\\d+ (hour|minute|day|week|month|year)s? ago', aria_label)\n",
    "                if time_match:\n",
    "                    video_posting_time.append(time_match.group())\n",
    "\n",
    "    # Close the webdriver\n",
    "    driver.quit()\n",
    "\n",
    "    return video_posting_time\n",
    "\n",
    "\n",
    "def write_to_csv(data, filename):\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write header row\n",
    "        writer.writerow(['Video URL', 'Thumbnail URL', 'Title', 'Views', 'Posting Time'])\n",
    "        # Write data rows\n",
    "        writer.writerows(data)\n",
    "\n",
    "\n",
    "def main():\n",
    "    channel_url = \"https://www.youtube.com/@PW-Foundation/videos\"\n",
    "\n",
    "    # Extract video URLs\n",
    "    video_urls = extract_video_urls(channel_url)\n",
    "\n",
    "    # Extract thumbnail URLs\n",
    "    thumbnail_urls = extract_thumbnail_urls(channel_url)\n",
    "\n",
    "    # Extract video titles\n",
    "    video_titles = extract_video_titles(channel_url)\n",
    "\n",
    "    # Extract video views\n",
    "    video_views = extract_video_views(channel_url)\n",
    "\n",
    "    # Extract video posting time\n",
    "    video_posting_time = extract_video_posting_time(channel_url)\n",
    "\n",
    "    # Create data rows for CSV\n",
    "    data = []\n",
    "    for index in range(5):\n",
    "        row = [video_urls[index], thumbnail_urls[index], video_titles[index], video_views[index],\n",
    "               video_posting_time[index]]\n",
    "        data.append(row)\n",
    "\n",
    "    # Write data to CSV file\n",
    "    write_to_csv(data, 'youtube_data.csv')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e59b38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
